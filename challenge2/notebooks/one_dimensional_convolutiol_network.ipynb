{"cells":[{"cell_type":"markdown","metadata":{"id":"eqLLlrl_ga1r"},"source":["Connect to Drive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XIEN0N88mIMf"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/gdrive')\n","%cd /gdrive/My Drive/ANNDL Challenge2"]},{"cell_type":"markdown","metadata":{"id":"TIiXZm8xKzWn"},"source":["# Summary\n","In this notebook is shown how we built a model based on a combination of one-dimensional convolutional layers and bidirectional LSTMs."]},{"cell_type":"markdown","metadata":{"id":"pQIRTdzkg2OV"},"source":["Import libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4jfnGMmJnQ5G"},"outputs":[],"source":["# Fix randomness and hide warnings\n","seed = 42\n","\n","import os\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n","os.environ['PYTHONHASHSEED'] = str(seed)\n","os.environ['MPLCONFIGDIR'] = os.getcwd()+'/configs/'\n","\n","import warnings\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","warnings.simplefilter(action='ignore', category=Warning)\n","\n","import numpy as np\n","np.random.seed(seed)\n","\n","import logging\n","\n","import random\n","random.seed(seed)\n","\n","\n","import matplotlib.pyplot as plt\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HLusodRrnSi5"},"outputs":[],"source":["# Import tensorflow\n","import tensorflow as tf\n","from tensorflow import keras as tfk\n","from tensorflow.keras import layers as tfkl\n","tf.autograph.set_verbosity(0)\n","tf.get_logger().setLevel(logging.ERROR)\n","tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n","tf.random.set_seed(seed)\n","tf.compat.v1.set_random_seed(seed)\n","print(tf.__version__)"]},{"cell_type":"markdown","metadata":{"id":"csmMvS8nhHc_"},"source":["Resources Paths"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uMGdnJmThIDC"},"outputs":[],"source":["trainDataFile = \"./fullTrainData.npy\"\n","validationDataFile = \"./fullValidationData.npy\""]},{"cell_type":"markdown","metadata":{"id":"M5AjiCsnjeWT"},"source":["Variables initialization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xnDEd_BinT7b"},"outputs":[],"source":["trainData = np.load(trainDataFile, allow_pickle=True)\n","validationData = np.load(validationDataFile, allow_pickle=True)\n","\n","# For each set we consider only the original series\n","labels = {'originalSeries': 0}"]},{"cell_type":"markdown","metadata":{"id":"IWyIc8atjopA"},"source":["### Manipulating the input dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GMhfDeeFo-ly"},"outputs":[],"source":["def build_sequences(series, window=300, stride=50, telescope=9):\n","  '''\n","  Split the single 'series' in multiple blocks of length 'window'. Each block is composed of a x part of length 'window' - 'telescope' and a y part of length 'telescope'.\n","  'Data\n","  'Window' is the length of the input of our network\n","  'Stride' is the number of samples to skip before starting the next window\n","  'Telescope' is the length of the output of our network\n","  '''\n","  blocks = []\n","  labels = []\n","  idx = 0\n","\n","  # We divide a time series in multiple blocks\n","  # If the series length is not a multiple of the window size, then the remaining slice of the series is skipped\n","  while(idx + window <= len(series)):\n","    blocks.append(series[idx : (idx + window - telescope)])\n","    labels.append(series[(idx + window - telescope) : (idx + window)])\n","    idx += stride\n","\n","  blocks = np.array(blocks)\n","  labels = np.array(labels)\n","  return np.array(blocks), np.array(labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ujkSHUzTm6qx"},"outputs":[],"source":["def compute_sequences_for_dataset(data, window, stride, telescope, minimum_length):\n","  '''\n","  Using the build_sequences function to increase the number of samples.\n","  'Data' is a list containing the starting series.\n","  'Window' is the length of the input of our network plus the telescope\n","  'Stride' is the number of samples to skip before starting the next window\n","  'Telescope' is the length of the output of our network\n","  'Minimum_length' is the minimum length that a series must have to be considered\n","  '''\n","\n","  x = []\n","  y = []\n","\n","  # Compute the various sequences for each stationary series in data\n","  for series in data:\n","\n","    # We skip the series with length less than the chosen 'minimum_length'\n","    if (len(series) >= minimum_length):\n","\n","      # If the series length is less than the window size we need to pad the series\n","      if (len(series) < window):\n","        padding_len = window - len(series)\n","\n","        # We isolate the Y portion (the telescope part) to let us adding the padding at the end of the X portion\n","        temp = series[ len(series) - telescope : len(series)]\n","\n","        # We isolate the X portion\n","        series  = series[0 : len(series) - telescope]\n","\n","        # We create the padding as a series full of 2-s\n","        padding = np.full((padding_len), 2, dtype= 'float32')\n","\n","        # Our resulting series is composed of the X portion, the padding and the Y portion\n","        series = np.concatenate((series, padding, temp))\n","\n","      # the X portions of the blocks and the Y portions of the blocks are computed\n","      x_new, y_new = build_sequences(series, window, stride, telescope)\n","\n","      for elem in x_new:\n","        x.append(elem)\n","      for elem in y_new:\n","        y.append(elem)\n","\n","  x = np.array(x)\n","  y = np.array(y)\n","\n","  return x, y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pq_fPtXFphMc"},"outputs":[],"source":["# Define common variables\n","\n","window = 218\n","telescope = 18\n","stride = 10"]},{"cell_type":"markdown","metadata":{"id":"BGHRChLTmnKj"},"source":["Using the build_sequences function to increase the number of samples in the training set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S2qusQ4-opDL"},"outputs":[],"source":["# Using the build_sequences function to increase the number of samples in the training set\n","\n","train_x = []\n","train_y = []\n","# In this case we consider only blocks with size equal to the window size without using any padding techniques. The time series (or the remaining slices) shorter than the window size are skipped.\n","train_x, train_y = compute_sequences_for_dataset(trainData[labels['originalSeries']], window, stride, telescope, window)\n","train_x = np.expand_dims(train_x, axis= 2)\n","\n","print(train_x.shape)\n","print(train_y.shape)\n","print(f\"Original number of sequences: {len(trainData[labels['originalSeries']])}\")\n","print(f\"Number of total sequences: {len(train_x)}\")\n","print(f'By choosing a window equal to {window} and stride equal to {stride}, there are {len(train_x) - len(trainData[labels[\"originalSeries\"]])} more time series')"]},{"cell_type":"markdown","metadata":{"id":"6W3ajhcLmpvQ"},"source":["Using the build_sequences function to increase the number of samples in the validation set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q_L63t5ApYon"},"outputs":[],"source":["# Using the build_sequences function to increase the number of samples in the validation set\n","\n","val_x = []\n","val_y = []\n","# In this case we consider only blocks with size equal to the window size without using any padding techniques. The time series (or the remaining slices) shorter than the window size are skipped.\n","val_x, val_y = compute_sequences_for_dataset(validationData[labels['originalSeries']], window, window, telescope, window)\n","val_x = np.expand_dims(val_x, axis= 2)\n","\n","print(val_x.shape)\n","print(val_y.shape)\n","print(f\"Original number of sequences: {len(validationData[labels['originalSeries']])}\")\n","print(f\"Number of total sequences: {len(val_x)}\")\n","print(f'By choosing a window equal to {window} and a stride equal to {stride}, there are {len(val_x) - len(validationData[labels[\"originalSeries\"]])} more time series')"]},{"cell_type":"markdown","metadata":{"id":"gybABmubp454"},"source":["### Developing the network"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nsjfQauppS5q"},"outputs":[],"source":["# Define common variables\n","input_shape = [window - telescope, 1]\n","output_shape = [telescope, 1]\n","batch_size = 64\n","epochs = 200\n","\n","save_the_model_on_file = False                                                  # Flag that says if it should save the model on file\n","model_file = \"./tmp/model\"                                                      # The model file path"]},{"cell_type":"markdown","metadata":{"id":"c9nBiuXeropA"},"source":["Define the network"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y0i5IGQmpUHl"},"outputs":[],"source":["def build_model(input_shape, output_shape):\n","\n","    input_layer = tfkl.Input(shape=input_shape)\n","    x = tfkl.Bidirectional(tfkl.LSTM(256, return_sequences=True, name='lstm'), name='bidirectional_lstm_1')(input_layer)\n","\n","    x = tfkl.Conv1D(128, 3, padding='same', activation='relu', name='conv1')(x)\n","    x = tfkl.MaxPooling1D()(x)\n","    x = tfkl.Bidirectional(tfkl.LSTM(128, return_sequences=True, name='lstm'), name='bidirectional_lstm_2')(x)\n","\n","    x = tfkl.Conv1D(128, 3, padding='same', activation='relu', name='conv2')(x)\n","    x = tfkl.MaxPooling1D()(x)\n","    x = tfkl.Bidirectional(tfkl.LSTM(128, return_sequences=True, name='lstm'), name='bidirectional_lstm_3')(x)\n","\n","    x = tfkl.Conv1D(128, 3, padding='same', activation='relu', name='conv3')(x)\n","    x = tfkl.MaxPooling1D()(x)\n","    x = tfkl.Bidirectional(tfkl.LSTM(128, return_sequences=True, name='lstm'), name='bidirectional_lstm_4')(x)\n","\n","    output_layer = tfkl.Conv1D(output_shape[1], 3, padding='same', name='output_layer')(x)\n","    crop_size = output_layer.shape[1] - output_shape[0]\n","    output_layer = tfkl.Cropping1D((0, crop_size), name='cropping')(output_layer)\n","\n","    model = tf.keras.Model(inputs=input_layer, outputs=output_layer, name='CONV_LSTM_model')\n","    model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=tf.keras.optimizers.Adam())\n","\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"Hl9klVXUr6WG"},"source":["Train the network"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E7Xg5FFepVju"},"outputs":[],"source":["# Create the model\n","model = build_model(input_shape, output_shape)\n","model.summary()\n","\n","# Train the model\n","history = model.fit(\n","    x = train_x,\n","    y = train_y,\n","    batch_size = batch_size,\n","    epochs = epochs,\n","    validation_data=(val_x, val_y),\n","    callbacks = [\n","        tfk.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=10, restore_best_weights=True),\n","        tfk.callbacks.ReduceLROnPlateau(monitor='val_loss', mode='min', patience=3, factor=0.9, min_lr=1e-5)\n","    ]\n",").history\n","\n","#Plot loss\n","best_epoch = np.argmin(history['val_loss'])\n","plt.figure(figsize=(17,4))\n","plt.plot(history['loss'], label='Training loss', alpha=.8, color='#ff7f0e')\n","plt.plot(history['val_loss'], label='Validation loss', alpha=.9, color='#5a9aa5')\n","plt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\n","plt.title('Mean Squared Error')\n","plt.legend()\n","plt.grid(alpha=.3)\n","plt.show()\n","\n","plt.figure(figsize=(18,3))\n","plt.plot(history['lr'], label='Learning Rate', alpha=.8, color='#ff7f0e')\n","plt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\n","plt.legend()\n","plt.grid(alpha=.3)\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gCym1fnrRLaW"},"outputs":[],"source":["if save_the_model_on_file:\n","  model.save(model_file)\n","  del model\n","\n","  model = tfk.models.load_model(model_file)"]},{"cell_type":"markdown","metadata":{"id":"eqYBpWQ3ukSu"},"source":["### Inferences on predictions"]},{"cell_type":"markdown","metadata":{"id":"odoP-IuWDbqK"},"source":["Inference the predictions and compute the MSE and MAE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UEgOxbOdKluH"},"outputs":[],"source":["predictions = model.predict(val_x)\n","\n","# Calculate and print Mean Squared Error (MSE)\n","mean_squared_error = tfk.metrics.mean_squared_error(val_y.flatten(), predictions.flatten()).numpy()\n","print(f\"Mean Squared Error: {mean_squared_error}\")\n","\n","# Calculate and print Mean Absolute Error (MAE)\n","mean_absolute_error = tfk.metrics.mean_absolute_error(val_y.flatten(), predictions.flatten()).numpy()\n","print(f\"Mean Absolute Error: {mean_absolute_error}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Buu0ds31N9hz"},"outputs":[],"source":["def inspect_timeseries_predictions(X, Y, preds, num, telescope):\n","    '''\n","    Randomly plot a number 'num' of series composed by the known x portion and the true y portion and the predicted portion.\n","    'X' is a list containing all the available x portions.\n","    'Y' is a list containing all the available y portions (the true).\n","    'Preds' is a list containing all the predicted y portions (the predictions).\n","    'Num' is the number of series to plot.\n","    'Telescope' is the length of each y portion (it is valid also for the length of each predicted portion).\n","    '''\n","\n","    figs, axs = plt.subplots(num, 1, sharex=True, figsize=(17,17))\n","    for i in range(0, num):\n","        idx=np.random.randint(0,len(X))\n","        axs[i].plot(np.arange(len(X[idx])), X[idx])\n","        axs[i].plot(np.arange(len(X[idx]), len(X[idx])+telescope), Y[idx], color='orange')\n","        axs[i].plot(np.arange(len(X[idx]), len(X[idx])+telescope), preds[idx], color='green')\n","        axs[i].set_ylim(-1,1)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IzQsvp3IOAHu"},"outputs":[],"source":["# Plot some predictions\n","inspect_timeseries_predictions(val_x, val_y, predictions, 10, telescope)"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}