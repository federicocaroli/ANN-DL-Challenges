{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1_Y55hiaYWO98LqnZwr4sSHBjVeQbmX7Y","timestamp":1699387112050}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["This notebook has been used to create from zero a model based on ResNet architecture. This model is NOT part of the submission on Codalab.\n","The peculiarities are:\n","- Based on ResNet architecture\n","- We applied resizing to 224x224\n","- We appliead rescaling to [0, 2]\n","- We didn't apply augmentation techniques during the training phase"],"metadata":{"id":"8TFwY85yQ-mf"}},{"cell_type":"code","source":["seed = 75\n","\n","import shutil\n","import os\n","\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n","os.environ['PYTHONHASHSEED'] = str(seed)\n","os.environ['MPLCONFIGDIR'] = os.getcwd()+'/configs/'\n","\n","import warnings\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","warnings.simplefilter(action='ignore', category=Warning)\n","\n","import numpy as np\n","np.random.seed(seed)\n","\n","import logging\n","\n","import random\n","random.seed(seed)\n","\n","import tensorflow as tf\n","from tensorflow import keras as tfk\n","from tensorflow.keras import layers as tfkl\n","from tensorflow.keras.models import Sequential\n","\n","from functools import partial\n","\n","tf.autograph.set_verbosity(0)\n","tf.get_logger().setLevel(logging.ERROR)\n","tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n","tf.random.set_seed(seed)\n","tf.compat.v1.set_random_seed(seed)\n","\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split, KFold\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score"],"metadata":{"id":"DJC7-cKE8gLL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset = np.load('clean_collection.npz', allow_pickle=True)\n","data = dataset['data']\n","labels = dataset['labels']\n","\n","# Cast string labels into integer values\n","casted_labels = []\n","for label in labels:\n","  if label == \"unhealthy\":\n","    casted_labels.append(1)\n","  elif label == \"healthy\":\n","    casted_labels.append(0)\n","  else:\n","    raise Exception(\"Invalid label\")\n","\n","# Expand the labels dimension moving from (x,) to (x, 1), with x cardinality\n","casted_labels = np.expand_dims(casted_labels, axis=-1)\n","\n","print(\"Training-Validation-Test Data Shape:\", data.shape)\n","print(\"Training-Validation-Test Casted Labels Shape:\", casted_labels.shape)\n","\n","# Inspect the target\n","print('Counting occurrences of target classes:')\n","unique, counts = np.unique(casted_labels, return_counts=True)\n","print(dict(zip(unique, counts)))\n","\n","# We split the dataset in training_and_validation set and test set\n","x_train_val, x_test, y_train_val, y_test = train_test_split(data, casted_labels, random_state=seed, test_size=500, stratify = casted_labels)\n","\n","# Print the shapes of the resulting datasets\n","print(\"Training-Validation Data Shape:\", x_train_val.shape)\n","print(\"Training-Validation Label Shape:\", y_train_val.shape)\n","print(\"Test Data Shape:\", x_test.shape)\n","print(\"Test Label Shape:\", y_test.shape)\n","\n","del dataset\n","del data\n","del labels\n","del unique\n","del counts"],"metadata":{"id":"5RFJFM9__A6K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img_augmentation = Sequential(\n","    [\n","        tfkl.RandomRotation(factor=0.15),\n","        tfkl.RandomTranslation(height_factor=0.1, width_factor=0.1),\n","        tfkl.RandomFlip(),\n","    ],\n","    name=\"img_augmentation\",\n",")"],"metadata":{"id":"tW544oYQas9I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the default conv. layer\n","DefaultConv2D = partial(tfkl.Conv2D, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=\"he_normal\",use_bias=False)\n","\n","class ResidualUnit(tfkl.Layer):\n","  def __init__(self, filters, strides=1, **kwargs):\n","    super().__init__()\n","\n","    # Main model layers\n","    self.main_layers=[\n","        DefaultConv2D(filters, strides=strides),\n","        tfkl.BatchNormalization(),\n","        tfkl.ReLU(),\n","        DefaultConv2D(filters),\n","        tfkl.BatchNormalization()\n","    ]\n","\n","    # Only if strides is greater than 1 we add other layers\n","    self.skip_layers=[]\n","    if strides > 1:\n","      self.skip_layers = [\n","          DefaultConv2D(filters, strides=strides, kernel_size=1),\n","          tfkl.BatchNormalization()\n","      ]\n","\n","  # Returns the layers\n","  def call(self, inputs):\n","    Z = inputs\n","    for layer in self.main_layers:\n","      Z = layer(Z)\n","    skip_Z = inputs\n","    for layer in self.skip_layers:\n","      skip_Z = layer(skip_Z)\n","    return tfkl.ReLU()(Z + skip_Z)\n"],"metadata":{"id":"jBnajvtU3Wdm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def build_model(denses, units):\n","\n","  # The input layer\n","  inputs = tfkl.Input(shape=(96, 96, 3))\n","\n","  # Perform augmentation (only during training!)\n","  x = img_augmentation(inputs)\n","\n","  # Resizing layer used to resize input images\n","  x = tfkl.Resizing(height = 224, width = 224, crop_to_aspect_ratio = True)(x)\n","\n","  # Rescaling layer used to rescale input images\n","  x = tfkl.Rescaling(scale = 2/255, offset = -1)(x)\n","\n","  # Let's build the network\n","\n","  x = DefaultConv2D(64, kernel_size = 7, strides = 2)(x)\n","  x = tfkl.BatchNormalization()(x)\n","  x = tfkl.ReLU()(x)\n","  x = tfkl.MaxPool2D(pool_size = 3, strides = 2, padding= \"same\")(x)\n","\n","  prev_filters = 64\n","  for filters in [64] * 2 + [128] * 2 + [256] * 2 + [512] * 2:\n","    strides = 1 if filters == prev_filters else 2\n","    ru = ResidualUnit(filters, strides=strides)\n","    x = ru(x)\n","    prev_filters = filters\n","\n","  # Last part\n","  x = tfkl.GlobalAveragePooling2D()(x)\n","  if(denses != 0):\n","\n","    # For each dense layer we specify the number of neurons, the kernel_initializer function, a batch normalization layer and the ReLU activation function\n","    for ind in range(1, denses + 1):\n","      x = tfkl.Dense(units=int(units/ind), kernel_initializer=tfk.initializers.HeUniform(seed=seed))(x)\n","      x = tfkl.BatchNormalization()(x)\n","      x = tfkl.ReLU()(x)\n","\n","  # The output layer\n","  outputs = tfkl.Dense(1, activation=\"sigmoid\", name=\"pred\")(x)\n","\n","  model = tf.keras.Model(inputs, outputs, name=\"model\")\n","\n","  model.compile(\n","    optimizer=tf.keras.optimizers.AdamW(), loss=tfk.losses.BinaryCrossentropy(), metrics=[\"accuracy\"]\n","  )\n","\n","  return model"],"metadata":{"id":"SYIZGcGHXZTU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Standard parameters\n","\n","# Define the number of folds for cross-validation\n","num_folds = 5\n","\n","# Initialize lists to store training histories, scores, and best epochs\n","scores = []\n","best_epochs = []\n","\n","# Create a KFold cross-validation object\n","kfold = KFold(n_splits=num_folds, shuffle=True, random_state=seed)\n","\n","lr_scheduler = tfk.callbacks.ReduceLROnPlateau(\n","  monitor='val_accuracy',                                                 # Metric to monitor (validation mean squared error in this case)\n","  patience=5,                                                             # Number of epochs with no improvement after which learning rate will be reduced\n","  factor=0.9,                                                             # Factor by which the learning rate will be reduced (0.9 in this case)\n","  mode='min',                                                             # Mode to decide when to reduce learning rate ('min' means reduce when metric stops decreasing)\n","  min_lr=1e-5                                                             # Minimum learning rate\n",")\n","\n","early_stopping = tfk.callbacks.EarlyStopping(\n","    monitor='val_accuracy',\n","    mode='max',\n","    patience=30,\n","    restore_best_weights=True\n",")\n","\n","batch_size = 64\n","epochs=300\n","\n","patience = 30\n","\n","callbacks = [early_stopping, lr_scheduler]\n","\n","# Loop through each fold\n","for fold_idx, (train_idx, valid_idx) in enumerate(kfold.split(x_train_val, y_train_val)):\n","\n","  print(\"Starting training on fold num: {}\".format(fold_idx+1))\n","\n","  indexTrainingTrue = []\n","  for i in range(0, len(y_train_val)):\n","    if i in train_idx:\n","      indexTrainingTrue.append(True)\n","    else:\n","      indexTrainingTrue.append(False)\n","\n","  # Build a new model for each fold\n","  model = build_model(1,64)\n","\n","  # Train the model on the training data for this fold and store the history\n","  history = model.fit(\n","    x = x_train_val[indexTrainingTrue],                                       # Select only the images that compose the training set\n","    y = y_train_val[indexTrainingTrue],                                       # Select only the images that compose the validation set\n","    validation_data=(x_train_val[np.logical_not(indexTrainingTrue)], y_train_val[np.logical_not(indexTrainingTrue)]),   # The same selection, but on the labels\n","    batch_size = batch_size,\n","    epochs = epochs,\n","    callbacks = callbacks,\n","  ).history\n","\n","  # Evaluate the model on the validation data for this fold\n","  score = model.evaluate(x_train_val[np.logical_not(indexTrainingTrue)], y_train_val[np.logical_not(indexTrainingTrue)], verbose=0)\n","  # Append the score\n","  scores.append(score[1])\n","\n","  # Calculate the best epoch for early stopping\n","  best_epoch = len(history['loss']) - patience\n","  best_epochs.append(best_epoch)"],"metadata":{"id":"5d4tKrRo93Zs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Print mean and standard deviation of MSE scores\n","print(f\"Scores: {scores}. Mean: {np.mean(scores).round(4)}. Std:  {np.std(scores).round(4)}\")\n","\n","# Calculate the average best epoch from K folds cross-validation\n","avg_epochs = int(np.mean(best_epochs))\n","print(f\"Best epochs: {best_epochs}. Best average epoch: {avg_epochs}\")"],"metadata":{"id":"aOYpQIdm7KRU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Final training of the model\n","\n","lr_scheduler = tfk.callbacks.ReduceLROnPlateau(\n","  monitor='val_accuracy',                                                 # Metric to monitor (validation mean squared error in this case)\n","  patience=5,                                                             # Number of epochs with no improvement after which learning rate will be reduced\n","  factor=0.9,                                                             # Factor by which the learning rate will be reduced (0.9 in this case)\n","  mode='min',                                                             # Mode to decide when to reduce learning rate ('min' means reduce when metric stops decreasing)\n","  min_lr=1e-5                                                             # Minimum learning rate\n",")\n","\n","batch_size = 64\n","\n","patience = 30\n","\n","# We split the training_and_validation set in training set and validation set\n","x_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val, random_state=seed, test_size=500, stratify = y_train_val)\n","\n","callbacks = [lr_scheduler]\n","\n","# Build a new model\n","model = build_model(1,64)\n","\n","# Train the model on the whole training set\n","history = model.fit(\n","  x = x_train,\n","  y = y_train,\n","  validation_data=(x_val, y_val),\n","  batch_size = batch_size,\n","  epochs = avg_epochs,\n","  callbacks = callbacks,\n",").history\n","\n","del avg_epochs, batch_size, callbacks, x_train, y_train"],"metadata":{"id":"da2cC58KqBks"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(15,5))\n","plt.plot(history['loss'], label='Training Loss', alpha=.3, color='#4D61E2', linestyle='--')\n","plt.plot(history['val_loss'], label='Validation Loss', alpha=.8, color='#4D61E2')\n","plt.legend(loc='upper left')\n","plt.title('Binary Crossentropy')\n","plt.grid(alpha=.3)\n","\n","plt.figure(figsize=(15,5))\n","plt.plot(history['accuracy'], label='Training Accuracy', alpha=.3, color='#4D61E2', linestyle='--')\n","plt.plot(history['val_accuracy'], label='Validation Accuracy', alpha=.8, color='#4D61E2')\n","plt.legend(loc='upper left')\n","plt.title('Accuracy')\n","plt.grid(alpha=.3)\n","\n","plt.show()\n","\n","del history"],"metadata":{"id":"x6swqgC8Vngd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Validation Accuracy, F1, Precision, Recall computation\n","\n","val_predictions = model.predict(x_val, verbose=0)\n","val_predictions = tf.where(val_predictions < 0.5, 0, 1)\n","val_predictions = val_predictions.numpy()\n","\n","# Compute scores\n","val_accuracy = accuracy_score(y_val, val_predictions)\n","val_f1 = f1_score(y_val, val_predictions)\n","val_precision = precision_score(y_val, val_predictions)\n","val_recall = recall_score(y_val, val_predictions)\n","\n","# Display the computed metrics\n","print('Val Accuracy:', val_accuracy.round(4))\n","print('Val F1:', val_f1.round(4))\n","print('Val Precision:', val_precision.round(4))\n","print('Val Recall:', val_recall.round(4))\n","\n","# Test Accuracy, F1, Precision, Recall computation\n","\n","test_predictions = model.predict(x_test, verbose=0)\n","test_predictions = tf.where(test_predictions < 0.5, 0, 1)\n","test_predictions = test_predictions.numpy()\n","\n","# Compute scores\n","test_accuracy = accuracy_score(y_test, test_predictions)\n","test_f1 = f1_score(y_test, test_predictions)\n","test_precision = precision_score(y_test, test_predictions)\n","test_recall = recall_score(y_test, test_predictions)\n","\n","# Display the computed metrics\n","print('Test Accuracy:', test_accuracy.round(4))\n","print('Test F1:', test_f1.round(4))\n","print('Test Precision:', test_precision.round(4))\n","print('Test Recall:', test_recall.round(4))\n","\n","# Save the model\n","model.save(\"ResNetFrom0\")"],"metadata":{"id":"rEbxhxKSElUo"},"execution_count":null,"outputs":[]}]}